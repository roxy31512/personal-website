<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Three Strategies to Address AI Information Gaps | Roxana Akhmetova</title>
  <meta name="description" content="Three prioritized strategies to address information gaps in AI: mandatory disclosure, liability-based economic pressure, and international coordination."/>
  <meta name="keywords" content="AI Governance, Policy Strategy, Regulation, Transparency, Capability Disclosure"/>
  <meta name="author" content="Roxana Akhmetova"/>

  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            dark: '#1a1a1a',
            mid: '#4d4d4d',
            light: '#f7f7f8',
            oxford: '#002147',
            'oxford-100': '#e6eefb',
            'oxford-600': '#234e85'
          }
        }
      }
    }
  </script>

  <link rel="stylesheet" href="/style.css"/>

  <style>
    html, body {
      background-color: #1a1a1a !important;
      min-height: 100%;
      color: #ffffff;
    }

    main {
      margin-top: 100px;
      max-width: 900px;
      padding: 0 40px;
      margin-left: auto;
      margin-right: auto;
      margin-bottom: 80px;
    }

    h1 {
      font-size: 2.2rem !important;
      margin-bottom: 24px !important;
      color: #ffffff !important;
      font-weight: 600 !important;
      border-bottom: 3px solid #e6eefb;
      display: inline-block;
      padding-bottom: 8px;
    }

    .post-date {
      color: #9fc0ff;
      font-size: 0.95rem;
      margin-bottom: 30px;
      font-weight: 500;
    }

    .post-content p {
      color: #d8e1eb;
      font-size: 1.05rem;
      line-height: 1.8;
      margin-bottom: 20px;
    }

    .post-content ol {
      color: #d8e1eb;
      font-size: 1.05rem;
      line-height: 1.8;
      margin-left: 1.15rem;
      margin-bottom: 20px;
    }

    .post-content li {
      margin-bottom: 12px;
    }

    .post-content strong {
      color: #ffffff;
    }

    .blog-post-tags {
      display: flex;
      gap: 10px;
      flex-wrap: wrap;
      margin-top: 30px;
    }

    .blog-tag {
      background: rgba(159, 192, 255, 0.1);
      color: #9fc0ff;
      padding: 5px 12px;
      border-radius: 15px;
      font-size: 0.85rem;
      font-weight: 500;
    }

    a.back-link {
      display: inline-block;
      color: #9fc0ff;
      margin-bottom: 40px;
      font-weight: 600;
      transition: all 0.3s ease;
    }

    a.back-link:hover {
      color: #ffffff;
      transform: translateX(-4px);
    }

    @media (max-width: 768px) {
      main { padding: 0 20px; }
      h1 { font-size: 1.8rem !important; }
      .post-content p, .post-content li { font-size: 1rem; }
    }
  </style>
</head>

<body>
  <custom-navbar></custom-navbar>

  <main>
    <a href="/blog.html" class="back-link">&larr; Back to Blog</a>

    <h1>Three Strategies to Address AI Information Gaps</h1>
    <div class="post-date">October 21, 2025</div>

    <div class="post-content">
      <p>
        The core challenge is building institutional capacity to enforce and process the information. I'd prioritise three strategies:
      </p>

      <ol>
        <li>
          <strong>Mandatory Disclosure:</strong><br>
          Labs above a certain compute threshold should submit capability models to regulators before major training runs.
          The implementation challenge is that regulatory agencies would need to hire ML researchers or contract third-party auditors
          with security clearances. Labs will likely claim trade secret protections, as they did with California's SB 1047.
          The regulatory framework needs authority to compel disclosure despite labs' proprietary nature—similar to how pharmaceutical
          companies disclose clinical trial data to the US FDA.
        </li>

        <li>
          <strong>Liability-Based Economic Pressure:</strong><br>
          Develop safe harbors where labs that share capability models with regulators receive liability protection if forecasts turn out wrong.
          Labs that conceal information would face strict liability for harms from capability surprises. Insurers would demand capability
          models to price risk so labs cannot deploy models without liability coverage. Insurers may lack expertise to evaluate AI risk,
          as we saw with pandemic insurance where underwriters mispriced tail risks.
        </li>

        <li>
          <strong>International Coordination:</strong><br>
          Design a US–China agreement to share capability model insights via a trusted intermediary. The implementation challenge is:
          who is the intermediary? If US-dominated (like the IAEA historically was), China won't trust it. If neutral (UN-affiliated?),
          both countries may fear intelligence leakage. A more realistic near-term path is coordination among the US, UK, and EU on disclosure
          standards. If these jurisdictions align on what capability model information labs must report, they create de facto global standards
          because most frontier labs need market access in these regions.
        </li>
      </ol>

      <p>
        I would deprioritise whistleblower protections and procurement conditions because whistleblowing is reactive rather than preventive,
        and procurement leverage is limited since most frontier AI developments aren't government-funded.
      </p>
    </div>

    <div class="blog-post-tags">
      <span class="blog-tag">AI Governance</span>
      <span class="blog-tag">Policy Strategy</span>
      <span class="blog-tag">Regulation</span>
      <span class="blog-tag">Transparency</span>
    </div>
  </main>

  <custom-footer></custom-footer>
  <script src="/components/navbar.js"></script>
  <script src="/components/footer.js"></script>

  <script>
    document.addEventListener('selectstart', e => e.preventDefault());
    document.addEventListener('contextmenu', e => e.preventDefault());
    document.addEventListener('keydown', e => {
      if (
        (e.ctrlKey && (e.key === 'c' || e.key === 'x' || e.key === 's' || e.key === 'u')) ||
        (e.metaKey && (e.key === 'c' || e.key === 'x' || e.key === 's' || e.key === 'u')) ||
        (e.ctrlKey && e.shiftKey && e.key === 'I') ||
        (e.ctrlKey && e.shiftKey && e.key === 'J') ||
        (e.ctrlKey && e.key === 'p')
      ) {
        e.preventDefault();
      }
    });
    document.body.style.userSelect = 'none';
  </script>
</body>
</html>
